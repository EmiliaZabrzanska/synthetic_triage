import streamlit as st
import pandas as pd
from synthcity.plugins import Plugins
from synthcity.metrics.eval import Metrics
from synthcity.plugins.core.dataloader import GenericDataLoader
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier
import shap
import matplotlib.pyplot as plt


#####
# Part 1: Synthetic data generation and model training 
#####

@st.cache_resource
def load_model():
    """
    Loads seed data, generates 10k synthetic samples, 
    and uses GridSearchCV to find the BEST Random Forest.
    """
    # 1. Load Seed Data
    try:
        seed_df = pd.read_csv("seed_data.csv")
    except FileNotFoundError:
        st.error("seed_data.csv not found!")
        st.stop()

    # 2. Generate Synthetic Data (10,000 rows)
    loader = GenericDataLoader(seed_df, target_column="outcome")
    syn_model = Plugins().get("ctgan") 
    # Note: We increase max_epochs slightly to ensure better learning from the larger seed
    syn_model.fit(loader) 
    
    print("Generating 10,000 synthetic samples...")
    synthetic_data = syn_model.generate(count=10000)
    synthetic_df = synthetic_data.dataframe()

    # 3. Prepare for Training
    X = synthetic_df.drop('outcome', axis=1)
    y = synthetic_df['outcome']

    # 4. Define the Parameter Grid
    # We test different combinations to find the best model
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'class_weight': ['balanced', 'balanced_subsample']
    }

    # 5. Run Grid Search
    print("Tuning model hyperparameters...")
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=0)
    grid_search.fit(X, y)

    # 6. Get the Best Model
    best_model = grid_search.best_estimator_
    
    print(f"Best Parameters Found: {grid_search.best_params_}")
    
    # Return the optimized model and data
    return best_model, seed_df.columns.drop('outcome'), seed_df, synthetic_df

#####
# Part 2: Streamlit App for Triage Prediction
#####

# Load the trained model (it will be cached after the first run)
# Note: it now returns the model AND the feature names
model, feature_names, seed_df, synthetic_df = load_model()

# Set up the title and a description
st.title("Synthetic Triage Prediction App")
st.write(
    "This app predicts a triage outcome based on patient symptoms. "
    "The model was trained on **100% synthetic data** generated by `synthcity`, "
    "from a **seed dataset**."
)
st.write("---")

# --- Create the input controls in the sidebar ---
st.sidebar.header("Enter Patient Symptoms:")

# 1. Age Slider
age = st.sidebar.slider("What is the patient's age?", 0, 100, 25)

# 2. Days Sick Slider
days_sick = st.sidebar.slider("How many days has the patient been sick?", 0, 14, 3)

# 3. NEW: Pain Severity Slider
pain_severity = st.sidebar.slider("On a scale of 0-10, what is the pain severity?", 0, 10, 2)

# 4. Yes/No Selectors
fever = st.sidebar.selectbox("Does the patient have a fever?", ("No", "Yes"))
cough = st.sidebar.selectbox("Does the patient have a cough?", ("No", "Yes"))
chest_pain = st.sidebar.selectbox("Is the patient experiencing chest pain?", ("No", "Yes"))
breathing_difficulty = st.sidebar.selectbox("Is the patient having breathing difficulty?", ("No", "Yes"))
has_asthma = st.sidebar.selectbox("Does the patient have a history of asthma?", ("No", "Yes"))


#####
# Part 3: Prediction and Display
#####

# 1. Convert text inputs to numbers (1s and 0s)
fever_int = 1 if fever == "Yes" else 0
cough_int = 1 if cough == "Yes" else 0
chest_pain_int = 1 if chest_pain == "Yes" else 0
breathing_difficulty_int = 1 if breathing_difficulty == "Yes" else 0
has_asthma_int = 1 if has_asthma == "Yes" else 0

# 2. Create a dictionary of the inputs
input_dict = {
    'age': age,
    'fever': fever_int,
    'cough': cough_int,
    'chest_pain': chest_pain_int,
    'breathing_difficulty': breathing_difficulty_int,
    'days_sick': days_sick,
    'has_asthma': has_asthma_int,
    'pain_severity': pain_severity
}

# 3. Create a DataFrame from the inputs
input_data = pd.DataFrame([input_dict], columns=feature_names)

# 4. Make the prediction and get probabilities
prediction = model.predict(input_data)[0]
prediction_proba = model.predict_proba(input_data)

# 5. Display the results
st.subheader("Model Recommendation")

# Use st.metric for a display
if prediction == 'go_to_a&e':
    st.metric(label="Predicted Outcome", value=prediction, delta="High Urgency", delta_color="off")
    st.error("Recommendation: Seek urgent medical attention.")
elif prediction == 'see_gp':
    st.metric(label="Predicted Outcome", value=prediction, delta="Medium Urgency", delta_color="off")
    st.warning("Recommendation: Contact a GP or local healthcare provider.")
else:
    st.metric(label="Predicted Outcome", value=prediction, delta="Low Urgency", delta_color="inverse")
    st.success("Recommendation: Monitor symptoms and use self-care.")

# Display the raw inputs
st.subheader("Patient Data Entered:")
st.dataframe(input_data)

# Display the confidence scores (this is a great addition)
st.subheader("Prediction Confidence Scores")
proba_df = pd.DataFrame(prediction_proba, columns=model.classes_)
proba_df_transposed = proba_df.T.rename(columns={0: 'Confidence'})
st.bar_chart(proba_df_transposed)

#####
# Part 4: Explainable AI (SHAP) 
#####

st.write("---")
st.subheader("Explainability (Why did the model say that?)")

if st.checkbox("Show detailed explanation"):
    with st.spinner("Calculating feature importance..."):
        
        # 1. Create a TreeExplainer
        explainer = shap.TreeExplainer(model)
        
        # 2. Calculate SHAP values
        shap_values = explainer.shap_values(input_data)
        
        # 3. Find the index of the predicted class
        prediction_index = list(model.classes_).index(prediction)
        
        # 4. Extract the values safely (Handling List vs. Array formats)
        if isinstance(shap_values, list):
            # Old SHAP behavior: List of arrays [Class 0, Class 1, Class 2]
            # We pick the array for our class (prediction_index), then the 0th row (our patient)
            shap_val_for_prediction = shap_values[prediction_index][0]
        else:
            # New SHAP behavior: Single 3D Array (Samples, Features, Classes)
            # We pick Sample 0, All Features (:), and our specific Class (prediction_index)
            # Note: Sometimes shape is just (Samples, Features) if binary.
            if len(shap_values.shape) == 3:
                shap_val_for_prediction = shap_values[0, :, prediction_index]
            else:
                # Fallback for binary cases where it might return just the positive class
                shap_val_for_prediction = shap_values[0]
        
        # 5. Create DataFrame for plotting
        explanation_df = pd.DataFrame({
            'Feature': feature_names,
            'Contribution': shap_val_for_prediction
        }).sort_values(by='Contribution', ascending=True)
        
        # 6. Plot
        fig, ax = plt.subplots(figsize=(8, 5))
        
        # Red = Pushing towards this outcome, Blue = Pushing away
        colors = ['#ff4b4b' if x > 0 else '#1f77b4' for x in explanation_df['Contribution']]
        
        ax.barh(explanation_df['Feature'], explanation_df['Contribution'], color=colors)
        ax.set_xlabel(f"Contribution to '{prediction}' Prediction")
        ax.set_title(f"Which symptoms pushed the result towards '{prediction}'?")
        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
        
        st.pyplot(fig)
        
        st.info(
            """
            **How to read this graph:**
            * **Red Bars (Right):** These symptoms **increased** the likelihood of this outcome.
            * **Blue Bars (Left):** These symptoms **decreased** the likelihood.
            """
        )

#####
# Part 5: Research Evaluation (The "Proof")
#####

st.write("---")
st.header("Research Evaluation Report")
st.write(
    "This section compares the **Real Seed Data** (loaded from `seed_data.csv`) "
    "with the **Synthetic Data** generated by `synthcity`."
)

if st.button("Run Data Quality Evaluation"):
    
    with st.spinner("Running evaluation metrics..."):
        
        # 1. Wrap our PRE-LOADED dataframes into DataLoaders
        # We don't need to re-generate anything!
        loader_real = GenericDataLoader(seed_df, target_column="outcome")
        loader_fake = GenericDataLoader(synthetic_df, target_column="outcome")

        # 2. Run the Metrics
        evaluation = Metrics.evaluate(
            loader_real,
            loader_fake,
            metrics={
                'stats': ['jensenshannon_distance', 'chi_squared_test', 'inverse_kl_divergence']
            }
        )
    
    # --- Display Results ---
    st.subheader("1. Statistical Similarity")
    
    # 1. Extract only the 'mean' column (the score)
    results_view = evaluation[['mean']].copy()
    
    # 2. Rename the column to make it friendly
    results_view.columns = ['Score']
    
    # 3. Rename the cryptic row names to readable English
    # Note: We use a dictionary to map the ugly ID to a pretty name
    row_mapping = {
        'stats.jensenshannon_distance.marginal': 'Jensen-Shannon Distance (Lower is Better)',
        'stats.chi_squared_test.marginal': 'Chi-Squared Test (P-Value > 0.05 is Good)',
        'stats.inverse_kl_divergence.marginal': 'Inverse KL Divergence (Higher is Better)',
        'stats.kolmogorov_smirnov_test.marginal': 'KS Test (P-Value > 0.05 is Good)'
    }
    
    # Rename the index using our mapping
    results_view = results_view.rename(index=row_mapping)
    
    # 4. Display the clean table
    st.table(results_view)

    # --- Visual Comparison ---
    st.subheader("2. Visual Distribution Check")
    st.write("Comparing 'Age' distribution: Real (Blue) vs. Synthetic (Orange)")
    
    fig, ax = plt.subplots()
    # We use density=True to compare shapes even though sample sizes are different (26 vs 5000)
    ax.hist(seed_df['age'], density=True, alpha=0.5, label='Real (Seed)', bins=10)
    ax.hist(synthetic_df['age'], density=True, alpha=0.5, label='Synthetic', bins=10)
    ax.legend()
    st.pyplot(fig)

    st.subheader("3. Clinical Safety Check (Confusion Matrix)")
    st.write(
        "This matrix shows where the model makes mistakes. "
        "The **X-axis** is what the Model Predicted. The **Y-axis** is the Truth (Seed Data)."
    )
    
    # Generate predictions on the REAL seed data to see if the synthetic model works on real people
    # This is called TSTR (Train on Synthetic, Test on Real)
    X_real = seed_df.drop('outcome', axis=1)
    y_real = seed_df['outcome']
    y_pred_real = model.predict(X_real)
    
    # Create the matrix
    cm = confusion_matrix(y_real, y_pred_real, labels=model.classes_)
    
    # Plot it
    fig_cm, ax_cm = plt.subplots()
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
    disp.plot(cmap='Blues', ax=ax_cm, values_format='d')
    
    # Improve readability
    plt.xticks(rotation=45)
    st.pyplot(fig_cm)
    
    st.warning(
        """
        **What to look for:**
        * **Diagonal Line:** These are correct predictions. We want dark blue here.
        * **Off-Diagonal:** These are errors.
        * **CRITICAL ERROR:** Look at the **'Go to A&E' row**. If there are numbers in the 'Self Care' column, 
        that means the model told a sick person to stay home. **That is dangerous.**
        """
    )